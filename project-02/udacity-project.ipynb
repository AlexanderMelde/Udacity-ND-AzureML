{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Operationalizing Machine Learning\n",
    "** Project 2 **\n",
    "[[View Rubric](https://review.udacity.com/#!/rubrics/2893/view)]\n",
    "\n",
    "## Initialization\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python 3.9.1\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "source": [
    "## Authentication"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipped granting rights because using azure environment provided by udacity\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Prepare Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "\n",
    "# Create TabularDataset using TabularDatasetFactory\n",
    "# https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py\n",
    "dataset_path = \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv\"\n",
    "# i download and import the _train.csv, so no further splitting is necessary\n",
    "ds = TabularDatasetFactory.from_delimited_files(path=dataset_path)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import clean_data\n",
    "\n",
    "# Use the clean_data function to clean your data.\n",
    "x, y = clean_data(ds)\n",
    "ds_clean = x.join(y)"
   ]
  },
  {
   "source": [
    "TODO Take a screenshot of “Registered Datasets” in ML Studio showing that Bankmarketing dataset available"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Automated ML Experiment\n",
    "\n",
    "<p>Run the experiment using  <em>Classification</em>, ensure <em>Explain best model</em> is checked. <br> On Exit criterion, reduce the default (3 hours) to 1 and reduce the <em>Concurrency </em> from default to 5 (this number should always be less than the number of the compute cluster) <br><br> Note: This process takes about 15 minutes and it runs about 5 minutes per iteration</p>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experiment using Automated ML\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "ws = Workspace.get(name=\"quick-starts-ws-128192\") # UPDATE THIS LINE WITH EACH NEW VM INSTANCE!\n",
    "exp = Experiment(workspace=ws, name=\"udacity-project\")\n",
    "\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "run = exp.start_logging()\n",
    "\n",
    "# DONT FORGET TO CLICK THE LOGIN LINK!\n",
    "\n",
    "# Set parameters for AutoMLConfig\n",
    "# NOTE: DO NOT CHANGE THE experiment_timeout_minutes PARAMETER OR YOUR INSTANCE WILL TIME OUT.\n",
    "# If you wish to run the experiment longer, you will need to run this notebook in your own\n",
    "# Azure tenant, which will incur personal costs.\n",
    "automl_config = AutoMLConfig(\n",
    "    experiment_timeout_minutes=30,\n",
    "    task=\"classification\",\n",
    "    primary_metric=\"accuracy\",\n",
    "    training_data=ds_clean,\n",
    "    label_column_name=\"y\",\n",
    "    n_cross_validations=3)\n",
    "\n",
    "\n",
    "# configure a compute cluster\n",
    "\n",
    "# Create compute cluster \"Standard_DS12_v2\" and min number of nodes = 1\n",
    "# https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=python\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpucluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)\n",
    "\n",
    "\n",
    "# use that cluster to run the experiment.\n",
    "\n",
    "# Submit automl run\n",
    "automl_run = exp.submit(config=automl_config) #TODO: compute_target = cpu_cluster\n",
    "RunDetails(automl_run).show()\n",
    "automl_run.wait_for_completion()\n"
   ]
  },
  {
   "source": [
    "TODO: Take a screenshot showing that the experiment is shown as completed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "## Deploy the best model\n",
    "After the experiment run completes, a summary of all the models and their metrics are shown, including explanations. The Best Model will be shown in the Details tab. In the Models tab, it will come up first (at the top). Make sure you select the best model for deployment.\n",
    "\n",
    "Deploying the Best Model will allow to interact with the HTTP API service and interact with the model by sending data over POST requests.\n",
    "\n",
    "1. Select the <strong>best</strong> model for deployment\n",
    "2. Deploy the model and enable \"Authentication\"\n",
    "3. Deploy the model using Azure Container Instance (ACI)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best model\n",
    "# Retrieve and save your best automl model.\n",
    "# Get your best run and save the model from that run.\n",
    "#best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_automl_run, best_automl_model = automl_run.get_output()\n",
    "best_automl_run_metrics = best_automl_run.get_metrics()\n",
    "\n",
    "#parameter_values = best_automl_run.get_details()['runDefinition']['arguments']\n",
    "\n",
    "#print('Best Run Id: ', best_automl_run.id)\n",
    "print('Accuracy:', best_automl_run_metrics['accuracy'])\n",
    "print('Metrics:', best_automl_run_metrics)\n",
    "#print('Inverse of regularization strength:',parameter_values[1])\n",
    "#print('Maximum number of iterations to converge:',parameter_values[3])\n",
    "print(\"Model\",best_automl_model)\n",
    "\n",
    "# save best model\n",
    "print(\"Files\", best_automl_run.get_file_names())\n",
    "# https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run(class)?view=azure-ml-py#download-file-name--output-file-path-none---validate-checksum-false-\n",
    "best_automl_run.download_file('outputs/model.pkl', output_file_path='best_automl_model.joblib')\n",
    "\n",
    "# register best model\n",
    "best_automl_model_reg = best_automl_run.register_model(model_name='best_automl_model', model_path='outputs/model.pkl')"
   ]
  },
  {
   "source": [
    "TODO: Take a screenshot of the best model after the experiment completes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Enable logging / Application Insights\n",
    "Now that the Best Model is deployed, enable Application Insights and retrieve logs. Although this is configurable at deploy time with a check-box, it is useful to be able to run code that will enable it for you.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure <code>az</code> is installed, as well as the Python SDK for Azure\n",
    "# Create a new virtual environment with Python3\n",
    "# Write and run code to enable Application Insights\n",
    "# Use the provided code <code>logs.py</code> to view the logs"
   ]
  },
  {
   "source": [
    "TODO: Take a screenshot showing that \"Application Insights\" is enabled in the Details tab of the endpoint.\n",
    "\n",
    "TODO: Take a screenshot showing logs by running the provided <code>logs.py</code> script"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Swagger Documentation\n",
    "In this step, you will consume the deployed model using Swagger.\n",
    "\n",
    "Azure provides a Swagger JSON file for deployed models. Head to the Endpoints section, and find your deployed model there, it should be the first one on the list.\n",
    "\n",
    "A few things you need to pay attention to:\n",
    "\n",
    "swagger.sh will download the latest Swagger container, and it will run it on port 80. If you don't have permissions for port 80 on your computer, update the script to a higher number (above 9000 is a good idea).\n",
    "\n",
    "serve.py will start a Python server on port 8000. This script needs to be right next to the downloaded swagger.json file. NOTE: this will not work if swagger.json is not on the same directory.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the swagger.json file\n",
    "# <p>Run the <code>swagger.sh</code> and <code>serve.py</code></p>\n",
    "# Interact with the swagger instance running with the documentation for the HTTP API of the model.\n",
    "# Display the contents of the API for the model"
   ]
  },
  {
   "source": [
    "TODO: Take a screenshot showing that swagger runs on localhost showing the HTTP API methods and responses for the model\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Consume model endpoints\n",
    "Once the model is deployed, use the endpoint.py script provided to interact with the trained model. In this step, you need to run the script, modifying both the scoring_uri and the key to match the key for your service and the URI that was generated after deployment.\n",
    "\n",
    "Hint: This URI can be found in the Details tab, above the Swagger URI.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <p>Modifying both the <code>scoring_uri</code> and the <code>key</code> to match the key for your service and the URI that was generated after deployment</p>\n",
    "\n",
    "# <p>Execute the <code>endpoint.py</code> file, the output should be similar to the following: <br><code>{\"result\": [\"yes\", \"no\"]}</code></p>\n",
    "\n"
   ]
  },
  {
   "source": [
    "<p>Take a screenshot showing that the<code>endpoint.py</code> script runs against the API producing JSON output from the model.</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Optional: Benchmarking\n",
    "The following is an optional step to benchmark the endpoint using Apache bench. You will not be graded on it but I encourage you to try it out.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the Apache Benchmark command-line tool installed and available in your path\n",
    "# <p>In the <code>endpoint.py</code>, replace the key and URI again</p>\n",
    "# <p>Run <code>endpoint.py</code>. A data.json file should appear</p>\n",
    "# <p>Run the <code>benchmark.sh</code> file. The output should look similar to the text below</p>"
   ]
  },
  {
   "source": [
    "TODO: Take a screenshot showing that Apache Benchmark (ab) runs against the HTTP API using authentication keys to retrieve performance results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    " ```\n",
    " This is ApacheBench, Version 2.3 <$Revision: 1843412 $>\n",
    " Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/\n",
    " Licensed to The Apache Software Foundation, http://www.apache.org/\n",
    "\n",
    " Benchmarking 8530a665-66f3-49c8-a953-b82a2d312917.eastus.azurecontainer.io (be patient)...INFO: POST header ==\n",
    " ---\n",
    " POST /score HTTP/1.0\n",
    " Content-length: 812\n",
    " Content-type: application/json\n",
    " Authorization: Bearer Agb3D23IygXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    " Host: 8530a665-66f3-49c8-a953-b82a2d312917.eastus.azurecontainer.io\n",
    " User-Agent: ApacheBench/2.3\n",
    " Accept: */*\n",
    "\n",
    "\n",
    " ---\n",
    " LOG: header received:\n",
    " HTTP/1.0 200 OK\n",
    " Content-Length: 33\n",
    " Content-Type: application/json\n",
    " Date: Thu, 30 Jul 2020 12:33:34 GMT\n",
    " Server: nginx/1.10.3 (Ubuntu)\n",
    " X-Ms-Request-Id: babfc511-a0f0-4ecb-a243-b3010a76b8b9\n",
    " X-Ms-Run-Function-Failed: False\n",
    "\n",
    " \"{\\\"result\\\": [\\\"yes\\\", \\\"no\\\"]}\"\n",
    " LOG: Response code = 200\n",
    " LOG: header received:\n",
    " HTTP/1.0 200 OK\n",
    " Content-Length: 33\n",
    " Content-Type: application/json\n",
    " Date: Thu, 30 Jul 2020 12:33:34 GMT\n",
    " Server: nginx/1.10.3 (Ubuntu)\n",
    " X-Ms-Request-Id: b48dd8da-0b4e-44fd-a1e5-04043bfa77f1\n",
    " X-Ms-Run-Function-Failed: False\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Create and publish a pipeline\n",
    "For this part of the project, you will use the Jupyter Notebook provided in the starter files. You must make sure to update the notebook to have the same keys, URI, dataset, cluster, and model names already created."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the Jupyter Notebook aml-pipelines-with-automated-machine-learning-step.ipynb to the Azure ML studio\n",
    "\n",
    "# Update all the variables that are noted to match your environment\n",
    "\n",
    "# Make sure a <code>config.json</code> has been downloaded and is available in the current working directory\n",
    "\n",
    "# Run through the cells\n",
    "\n",
    "# Verify the pipeline has been created and shows in Azure ML studio, in the <em>Pipelines</em> section\n",
    "\n",
    "# Verify that the pipeline has been scheduled to run or is running\n"
   ]
  },
  {
   "source": [
    "TODO: Please take the following screenshots to show your work:\n",
    "- The pipeline section of Azure ML studio, showing that the pipeline has been created\n",
    "- The pipelines section in Azure ML Studio, showing the Pipeline Endpoint\n",
    "- The Bankmarketing dataset with the AutoML module\n",
    "- The “Published Pipeline overview”, showing a REST endpoint and a status of ACTIVE\n",
    "- In Jupyter Notebook, showing that the “Use RunDetails Widget” shows the step runs\n",
    "- In ML studio showing the scheduled run\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Documentation\n",
    "\n",
    "### Screencast\n",
    "In this project, you need to record a screencast that shows the entire process of the working ML application. The screencast should meet the following criteria: 1-5 min lenght, clear and understandable audio, at least full hd 16:9, readable text.\n",
    "\n",
    "In this project, you need to record a screencast that shows the entire process of the working ML application. The screencast should meet the following criteria:\n",
    "- Working deployed ML model endpoint\n",
    "- deployed pipeline\n",
    "- available automl model\n",
    "- Successful API requests to the endpoint with a JSON payload\n",
    "\n",
    "In case you are unable to provide an audio file, you can include a written description of your script instead of audio, if you prefer. Please include it in your README file.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert link to youtube here"
   ]
  },
  {
   "source": [
    "## Readme\n",
    "An important part of your project submissions is a README file that describes the project and documents the main steps. Please use the README.md template provided to you as a start. The README should include the following areas:\n",
    "\n",
    "- project overview\n",
    "- architectural diagram\n",
    "- short description how to improve project in the future\n",
    "- all screenshots mentioned above with short descriptions\n",
    "- link to the screencast video on youtube (or similar)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert link to readme here"
   ]
  },
  {
   "source": [
    "## Cleanup\n",
    "Not required but included because i think it's important"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cluster.delete()"
   ]
  }
 ]
}