{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Operationalizing Machine Learning\n",
    "** Project 2 **\n",
    "[[View Rubric](https://review.udacity.com/#!/rubrics/2893/view)]\n",
    "\n",
    "## Initialization\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#  Not needed when running notebook on azure:\n",
    "!pip install --upgrade -q -r requirements.txt\n",
    "!python --version"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python 3.9.1\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import pkg_resources\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import azureml.core\n",
    "\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.run import Run\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, TrainingOutput\n",
    "from azureml.pipeline.core.run import PipelineRun\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"Azure SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "source": [
    "## Authentication"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipped granting rights because using azure environment provided by udacity\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Azure Initialization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "# ws = Workspace.get(name=\"quick-starts-ws-128192\") # UPDATE THIS LINE WITH EACH NEW VM INSTANCE!\n",
    "\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "# DONT FORGET TO CLICK THE LOGIN LINK!"
   ]
  },
  {
   "source": [
    "## Prepare Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load the dataset from the Workspace. Otherwise, create it from the file\n",
    "found = False\n",
    "ds_key = \"Bank-marketing\"\n",
    "\n",
    "if ds_key in ws.datasets.keys(): \n",
    "        found = True\n",
    "        ds = ws.datasets[ds_key] \n",
    "\n",
    "if not found:\n",
    "        # Create AML Dataset and register it into Workspace\n",
    "        # Create TabularDataset using TabularDatasetFactory\n",
    "        # https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py  \n",
    "        # i download and import the _train.csv, so no further splitting is necessary\n",
    "        example_data = 'https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv'\n",
    "        ds = TabularDatasetFactory.from_delimited_files(path=dataset_path)  \n",
    "        #Register Dataset in Workspace\n",
    "        ds = ds.register(workspace=ws,\n",
    "                        name=ds_key,\n",
    "                        description=\"Bank Marketing DataSet for Udacity Course 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning like in project-01\n",
    "\n",
    "def clean_data(data):\n",
    "    # Dict for cleaning data\n",
    "    months = {\"jan\":1, \"feb\":2, \"mar\":3, \"apr\":4, \"may\":5, \"jun\":6, \"jul\":7, \"aug\":8, \"sep\":9, \"oct\":10, \"nov\":11, \"dec\":12}\n",
    "    weekdays = {\"mon\":1, \"tue\":2, \"wed\":3, \"thu\":4, \"fri\":5, \"sat\":6, \"sun\":7}\n",
    "\n",
    "    # Clean and one hot encode data\n",
    "    x_df = data.to_pandas_dataframe().dropna()\n",
    "    jobs = pd.get_dummies(x_df.job, prefix=\"job\")\n",
    "    x_df.drop(\"job\", inplace=True, axis=1)\n",
    "    x_df = x_df.join(jobs)\n",
    "    x_df[\"marital\"] = x_df.marital.apply(lambda s: 1 if s == \"married\" else 0)\n",
    "    x_df[\"default\"] = x_df.default.apply(lambda s: 1 if s == \"yes\" else 0)\n",
    "    x_df[\"housing\"] = x_df.housing.apply(lambda s: 1 if s == \"yes\" else 0)\n",
    "    x_df[\"loan\"] = x_df.loan.apply(lambda s: 1 if s == \"yes\" else 0)\n",
    "    contact = pd.get_dummies(x_df.contact, prefix=\"contact\")\n",
    "    x_df.drop(\"contact\", inplace=True, axis=1)\n",
    "    x_df = x_df.join(contact)\n",
    "    education = pd.get_dummies(x_df.education, prefix=\"education\")\n",
    "    x_df.drop(\"education\", inplace=True, axis=1)\n",
    "    x_df = x_df.join(education)\n",
    "    x_df[\"month\"] = x_df.month.map(months)\n",
    "    x_df[\"day_of_week\"] = x_df.day_of_week.map(weekdays)\n",
    "    x_df[\"poutcome\"] = x_df.poutcome.apply(lambda s: 1 if s == \"success\" else 0)\n",
    "\n",
    "    y_df = x_df.pop(\"y\").apply(lambda s: 1 if s == \"yes\" else 0)\n",
    "\n",
    "    return x_df, y_df\n",
    "\n",
    "found_clean = False\n",
    "if ds_key +\"-clean\" in ws.datasets.keys(): \n",
    "        found_clean = True\n",
    "        ds_clean = ws.datasets[ds_key +\"-clean\"] \n",
    "\n",
    "if not found_clean:\n",
    "    # Use the clean_data function to clean your data.\n",
    "    x, y = clean_data(ds)\n",
    "    df_clean = x.join(y)\n",
    "\n",
    "    #Register cleaned Dataset in Workspace\n",
    "    ds_clean = TabularDatasetFactory.register_pandas_dataframe(df_clean, ws.get_default_datastore(), ds_key +\"-clean\",\n",
    "                                                                description=\"Cleaned Bank Marketing DataSet for Udacity Course 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_clean = ds_clean.to_pandas_dataframe()\n",
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head(5)"
   ]
  },
  {
   "source": [
    "Screenshot of “Registered Datasets” in ML Studio showing that Bankmarketing dataset is available:\n",
    "![registered_datasets](images/registered_dataset.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Automated ML Experiment\n",
    "\n",
    "<p>Run the experiment using  <em>Classification</em>, ensure <em>Explain best model</em> is checked. <br> On Exit criterion, reduce the default (3 hours) to 1 and reduce the <em>Concurrency </em> from default to 5 (this number should always be less than the number of the compute cluster) <br><br> Note: This process takes about 15 minutes and it runs about 5 minutes per iteration</p>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experiment using Automated ML\n",
    "\n",
    "experiment_name = 'ml-experiment-1'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "# run = exp.start_logging()"
   ]
  },
  {
   "source": [
    "configure a compute cluster\n",
    "\n",
    "Create compute cluster \"Standard_DS12_v2\" and min number of nodes = 1\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=python"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"auto-ml\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',# for GPU, use \"STANDARD_NC6\"\n",
    "                                                           #vm_priority = 'lowpriority', # optional\n",
    "                                                           max_nodes=4)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True) # , min_node_count = 1, timeout_in_minutes = 10\n",
    "# For a more detailed view of current AmlCompute status, use get_status().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for AutoMLConfig\n",
    "# NOTE: DO NOT CHANGE THE experiment_timeout_minutes PARAMETER OR YOUR INSTANCE WILL TIME OUT.\n",
    "# If you wish to run the experiment longer, you will need to run this notebook in your own\n",
    "# Azure tenant, which will incur personal costs.\n",
    "\n",
    "automl_config = AutoMLConfig(compute_target=compute_target,\n",
    "                             task = \"classification\",\n",
    "                             training_data=ds_clean,\n",
    "                             label_column_name=\"y\",   \n",
    "                             path = './pipeline-project',\n",
    "                             enable_early_stopping= True,\n",
    "                             featurization= 'auto',\n",
    "                             debug_log = \"automl_errors.log\",\n",
    "                             experiment_timeout_minutes = 20,\n",
    "                             max_concurrent_iterations = 5,\n",
    "                             # n_cross_validations=3,\n",
    "                             primary_metric = \"AUC_weighted\" #  or \"accuracy\"\n",
    "                            )\n",
    "\n",
    "\n",
    "metrics_output_name = 'metrics_output'\n",
    "metrics_data = PipelineData(name='metrics_data',\n",
    "                           datastore=ws.get_default_datastore(),\n",
    "                           pipeline_output_name=metrics_output_name,\n",
    "                           training_output=TrainingOutput(type='Metrics'))\n",
    "\n",
    "best_model_output_name = 'best_model_output'\n",
    "best_model_data = PipelineData(name='model_data',\n",
    "                           datastore=ws.get_default_datastore(),\n",
    "                           pipeline_output_name=best_model_output_name,\n",
    "                           training_output=TrainingOutput(type='Model'))\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    description=\"pipeline_with_automlstep\",\n",
    "    workspace=ws,    \n",
    "    steps=[AutoMLStep(\n",
    "            name='automl_module',\n",
    "            automl_config=automl_config,\n",
    "            outputs=[metrics_data, best_model_data],\n",
    "            allow_reuse=True)\n",
    "          ])\n",
    "\n",
    "pipeline_run = exp.submit(pipeline) #TODO: compute_target = cpu_cluster #config=automl_config\n",
    "\n",
    "# Submit automl run\n",
    "RunDetails(pipeline_run).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion()\n"
   ]
  },
  {
   "source": [
    "Screenshot showing that the experiment is shown as completed:\n",
    "![experiment overview](images/experiments_overview.jpg)\n",
    "![completed run](images/completed_run.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "## Deploy the best model\n",
    "After the experiment run completes, a summary of all the models and their metrics are shown, including explanations. The Best Model will be shown in the Details tab. In the Models tab, it will come up first (at the top). Make sure you select the best model for deployment.\n",
    "\n",
    "Deploying the Best Model will allow to interact with the HTTP API service and interact with the model by sending data over POST requests.\n",
    "\n",
    "1. Select the <strong>best</strong> model for deployment\n",
    "2. Deploy the model and enable \"Authentication\"\n",
    "3. Deploy the model using Azure Container Instance (ACI)\n",
    "\n",
    "Docs: [PipelineRun Class](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinerun?view=azure-ml-py)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download pipeline output about metrics (of child runs) and examine them\n",
    "metrics_portref = pipeline_run.get_pipeline_output(metrics_output_name)\n",
    "num_file_downloaded = metrics_portref.download('.', show_progress=True)\n",
    "\n",
    "with open(metrics_portref._path_on_datastore) as f:\n",
    "    metrics = f.read()\n",
    "    \n",
    "pd.DataFrame(json.loads(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pipeline output about the best model and examine it\n",
    "best_model_portref = pipeline_run.get_pipeline_output(best_model_output_name)\n",
    "num_file_downloaded = best_model_portref.download('.', show_progress=True)\n",
    "\n",
    "with open(best_model_portref._path_on_datastore, \"rb\" ) as f:\n",
    "    best_model = pickle.load(f)\n",
    "\n",
    "# show best model\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register best model\n",
    "# best_automl_model_reg = best_automl_run.register_model(model_name='best_automl_model', model_path=best_model_portref._path_on_datastore) #'outputs/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine metrics of best model\n",
    "# print('Best Run Id: ', best_automl_run.id)\n",
    "# print('Accuracy:', best_automl_run_metrics['accuracy'])\n",
    "# print('Metrics:', best_automl_run_metrics)\n",
    "# print(\"Model\",best_automl_model)"
   ]
  },
  {
   "source": [
    "Screenshot of the best model after the experiment completes:\n",
    "![best model](images/best_model.jpg)\n",
    "![best model steps](images/best_model_steps.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Testing the best model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "ds_test = TabularDatasetFactory.from_delimited_files(path='https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv')\n",
    "\n",
    "x, y = clean_data(ds_test)\n",
    "df_test = x.join(y)\n",
    "df_test = df_test[pd.notnull(df_test['y'])]\n",
    "\n",
    "y_test = df_test['y']\n",
    "X_test = df_test.drop(['y'], axis=1)\n",
    "\n",
    "# predict\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Visualize via confusion matrix\n",
    "pd.DataFrame(confusion_matrix(y_test, y_test_pred)).style.background_gradient(cmap='Blues', low=0, high=0.9)"
   ]
  },
  {
   "source": [
    "## Deployment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name=\"Bankmarketing Train\", description=\"Training bankmarketing pipeline\", version=\"1.0\")\n",
    "\n",
    "published_pipeline\n"
   ]
  },
  {
   "source": [
    "Authenticate once again, to retrieve the `auth_header` so that the endpoint can be used"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_header = InteractiveLoginAuthentication().get_authentication_header()"
   ]
  },
  {
   "source": [
    "## Enable logging / Application Insights\n",
    "Now that the Best Model is deployed, enable Application Insights and retrieve logs. Although this is configurable at deploy time with a check-box, it is useful to be able to run code that will enable it for you.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure <code>az</code> is installed, as well as the Python SDK for Azure\n",
    "# Create a new virtual environment with Python3\n",
    "# Write and run code to enable Application Insights\n",
    "# Use the provided code <code>logs.py</code> to view the logs"
   ]
  },
  {
   "source": [
    "TODO: Take a screenshot showing that \"Application Insights\" is enabled in the Details tab of the endpoint.\n",
    "\n",
    "TODO: Take a screenshot showing logs by running the provided <code>logs.py</code> script"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Swagger Documentation\n",
    "In this step, you will consume the deployed model using Swagger.\n",
    "\n",
    "Azure provides a Swagger JSON file for deployed models. Head to the Endpoints section, and find your deployed model there, it should be the first one on the list.\n",
    "\n",
    "A few things you need to pay attention to:\n",
    "\n",
    "swagger.sh will download the latest Swagger container, and it will run it on port 80. If you don't have permissions for port 80 on your computer, update the script to a higher number (above 9000 is a good idea).\n",
    "\n",
    "serve.py will start a Python server on port 8000. This script needs to be right next to the downloaded swagger.json file. NOTE: this will not work if swagger.json is not on the same directory.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the swagger.json file\n",
    "# <p>Run the <code>swagger.sh</code> and <code>serve.py</code></p>\n",
    "# Interact with the swagger instance running with the documentation for the HTTP API of the model.\n",
    "# Display the contents of the API for the model"
   ]
  },
  {
   "source": [
    "TODO: Take a screenshot showing that swagger runs on localhost showing the HTTP API methods and responses for the model\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Consume model endpoints\n",
    "Once the model is deployed, use the endpoint.py script provided to interact with the trained model. In this step, you need to run the script, modifying both the scoring_uri and the key to match the key for your service and the URI that was generated after deployment.\n",
    "\n",
    "Hint: This URI can be found in the Details tab, above the Swagger URI.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for the web service, should be similar to:\n",
    "# 'http://8530a665-66f3-49c8-a953-b82a2d312917.eastus.azurecontainer.io/score' #URI that was generated after deployment\n",
    "scoring_uri = published_pipeline.endpoint # ''\n",
    "# If the service is authenticated, set the key or token # key for your service\n",
    "key = auth_header # ''\n",
    "\n",
    "# Two sets of data to score, so we get two results back\n",
    "data = {\"data\":\n",
    "        [\n",
    "          {\n",
    "            \"age\": 17,\n",
    "            \"campaign\": 1,\n",
    "            \"cons.conf.idx\": -46.2,\n",
    "            \"cons.price.idx\": 92.893,\n",
    "            \"contact\": \"cellular\",\n",
    "            \"day_of_week\": \"mon\",\n",
    "            \"default\": \"no\",\n",
    "            \"duration\": 971,\n",
    "            \"education\": \"university.degree\",\n",
    "            \"emp.var.rate\": -1.8,\n",
    "            \"euribor3m\": 1.299,\n",
    "            \"housing\": \"yes\",\n",
    "            \"job\": \"blue-collar\",\n",
    "            \"loan\": \"yes\",\n",
    "            \"marital\": \"married\",\n",
    "            \"month\": \"may\",\n",
    "            \"nr.employed\": 5099.1,\n",
    "            \"pdays\": 999,\n",
    "            \"poutcome\": \"failure\",\n",
    "            \"previous\": 1\n",
    "          },\n",
    "          {\n",
    "            \"age\": 87,\n",
    "            \"campaign\": 1,\n",
    "            \"cons.conf.idx\": -46.2,\n",
    "            \"cons.price.idx\": 92.893,\n",
    "            \"contact\": \"cellular\",\n",
    "            \"day_of_week\": \"mon\",\n",
    "            \"default\": \"no\",\n",
    "            \"duration\": 471,\n",
    "            \"education\": \"university.degree\",\n",
    "            \"emp.var.rate\": -1.8,\n",
    "            \"euribor3m\": 1.299,\n",
    "            \"housing\": \"yes\",\n",
    "            \"job\": \"blue-collar\",\n",
    "            \"loan\": \"yes\",\n",
    "            \"marital\": \"married\",\n",
    "            \"month\": \"may\",\n",
    "            \"nr.employed\": 5099.1,\n",
    "            \"pdays\": 999,\n",
    "            \"poutcome\": \"failure\",\n",
    "            \"previous\": 1\n",
    "          },\n",
    "      ]\n",
    "    }\n",
    "# Convert to JSON string\n",
    "input_data = json.dumps(data)\n",
    "with open(\"data.json\", \"w\") as _f:\n",
    "    _f.write(input_data)\n",
    "\n",
    "# Set the content type\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "# If authentication is enabled, set the authorization header\n",
    "headers['Authorization'] = f'Bearer {key}'\n",
    "\n",
    "# Make the request and display the response\n",
    "resp = requests.post(scoring_uri, input_data, headers=headers)\n",
    "print(resp.json())\n",
    "\n",
    "# output should be similar to this: {\"result\": [\"yes\", \"no\"]}"
   ]
  },
  {
   "source": [
    "**Alternatively trigger a run via notebook:**\n",
    "\n",
    "Get the REST url from the endpoint property of the published pipeline object. You can also find the REST url in your workspace in the portal. Build an HTTP POST request to the endpoint, specifying your authentication header. Additionally, add a JSON payload object with the experiment name and the batch size parameter. As a reminder, the process_count_per_node is passed through to ParallelRunStep because you defined it is defined as a PipelineParameter object in the step configuration.\n",
    "\n",
    "Make the request to trigger the run. Access the Id key from the response dict to get the value of the run id.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=auth_header, \n",
    "                         json={\"ExperimentName\": \"pipeline-rest-endpoint\"}\n",
    "                        )\n",
    "try:\n",
    "    response.raise_for_status()\n",
    "except Exception:    \n",
    "    raise Exception(\"Received bad response from the endpoint: {}\\n\"\n",
    "                    \"Response Code: {}\\n\"\n",
    "                    \"Headers: {}\\n\"\n",
    "                    \"Content: {}\".format(rest_endpoint, response.status_code, response.headers, response.content))\n",
    "\n",
    "run_id = response.json().get('Id')\n",
    "print('Submitted pipeline run: ', run_id)\n",
    "\n",
    "# Use run id to monitor status of new run. This will take 10-15 min, looks similar to previous pipeline run, so you can skip watching full output.\n",
    "published_pipeline_run = PipelineRun(ws.experiments[\"pipeline-rest-endpoint\"], run_id)\n",
    "RunDetails(published_pipeline_run).show()"
   ]
  },
  {
   "source": [
    "TODO: Take a screenshot showing that the `endpoint.py` script runs against the API producing JSON output from the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Optional: Benchmarking\n",
    "The following is an optional step to benchmark the endpoint using Apache bench. You will not be graded on it but I encourage you to try it out.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the Apache Benchmark command-line tool installed and available in your path\n",
    "# <p>In the <code>endpoint.py</code>, replace the key and URI again</p>\n",
    "# <p>Run <code>endpoint.py</code>. A data.json file should appear</p>\n",
    "# <p>Run the <code>benchmark.sh</code> file. The output should look similar to the text below</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ab -n 10 -v 4 -p data.json -T 'application/json' -H 'Authorization: Bearer REPLACE_WITH_KEY' http://REPLACE_WITH_API_URL/score"
   ]
  },
  {
   "source": [
    "TODO: Take a screenshot showing that Apache Benchmark (ab) runs against the HTTP API using authentication keys to retrieve performance results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Run Apache Benchmark for 10 times, producing output similar to:\n",
    "\n",
    " ```\n",
    " This is ApacheBench, Version 2.3 <$Revision: 1843412 $>\n",
    " Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/\n",
    " Licensed to The Apache Software Foundation, http://www.apache.org/\n",
    "\n",
    " Benchmarking 8530a665-66f3-49c8-a953-b82a2d312917.eastus.azurecontainer.io (be patient)...INFO: POST header ==\n",
    " ---\n",
    " POST /score HTTP/1.0\n",
    " Content-length: 812\n",
    " Content-type: application/json\n",
    " Authorization: Bearer Agb3D23IygXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    " Host: 8530a665-66f3-49c8-a953-b82a2d312917.eastus.azurecontainer.io\n",
    " User-Agent: ApacheBench/2.3\n",
    " Accept: */*\n",
    "\n",
    "\n",
    " ---\n",
    " LOG: header received:\n",
    " HTTP/1.0 200 OK\n",
    " Content-Length: 33\n",
    " Content-Type: application/json\n",
    " Date: Thu, 30 Jul 2020 12:33:34 GMT\n",
    " Server: nginx/1.10.3 (Ubuntu)\n",
    " X-Ms-Request-Id: babfc511-a0f0-4ecb-a243-b3010a76b8b9\n",
    " X-Ms-Run-Function-Failed: False\n",
    "\n",
    " \"{\\\"result\\\": [\\\"yes\\\", \\\"no\\\"]}\"\n",
    " LOG: Response code = 200\n",
    " LOG: header received:\n",
    " HTTP/1.0 200 OK\n",
    " Content-Length: 33\n",
    " Content-Type: application/json\n",
    " Date: Thu, 30 Jul 2020 12:33:34 GMT\n",
    " Server: nginx/1.10.3 (Ubuntu)\n",
    " X-Ms-Request-Id: b48dd8da-0b4e-44fd-a1e5-04043bfa77f1\n",
    " X-Ms-Run-Function-Failed: False\n",
    "\n",
    " \n",
    "# \"{\\\"result\\\": [\\\"yes\\\", \\\"no\\\"]}\"\n",
    "# LOG: Response code = 200\n",
    "# LOG: header received:\n",
    "# HTTP/1.0 200 OK\n",
    "# Content-Length: 33\n",
    "# Content-Type: application/json\n",
    "# Date: Thu, 30 Jul 2020 12:33:34 GMT\n",
    "# Server: nginx/1.10.3 (Ubuntu)\n",
    "# X-Ms-Request-Id: b48dd8da-0b4e-44fd-a1e5-04043bfa77f1\n",
    "# X-Ms-Run-Function-Failed: False\n",
    "#\n",
    "# \"{\\\"result\\\": [\\\"yes\\\", \\\"no\\\"]}\"\n",
    "# LOG: Response code = 200\n",
    "# ..done\n",
    "#\n",
    "#\n",
    "# Server Software:        nginx/1.10.3\n",
    "# Server Hostname:        8530a665-66f3-49c8-a953-b82a2d312917.eastus.azurecontainer.io\n",
    "# Server Port:            80\n",
    "#\n",
    "# Document Path:          /score\n",
    "# Document Length:        33 bytes\n",
    "#\n",
    "# Concurrency Level:      1\n",
    "# Time taken for tests:   1.599 seconds\n",
    "# Complete requests:      10\n",
    "# Failed requests:        0\n",
    "# Total transferred:      2600 bytes\n",
    "# Total body sent:        10560\n",
    "# HTML transferred:       330 bytes\n",
    "# Requests per second:    6.25 [#/sec] (mean)\n",
    "# Time per request:       159.918 [ms] (mean)\n",
    "# Time per request:       159.918 [ms] (mean, across all concurrent requests)\n",
    "# Transfer rate:          1.59 [Kbytes/sec] received\n",
    "#                         6.45 kb/s sent\n",
    "#                         8.04 kb/s total\n",
    "#\n",
    "# Connection Times (ms)\n",
    "#               min  mean[+/-sd] median   max\n",
    "# Connect:       21   23   0.8     23      24\n",
    "# Processing:    92  137  28.3    151     176\n",
    "# Waiting:       92  137  28.3    151     176\n",
    "# Total:        114  160  28.0    172     199#\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Summary: Create and publish a pipeline\n",
    "You must make sure to update the notebook to have the same keys, URI, dataset, cluster, and model names already created."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# upload the Jupyter Notebook aml-pipelines-with-automated-machine-learning-step.ipynb to the Azure ML studio\n",
    "\n",
    "# Update all the variables that are noted to match your environment\n",
    "\n",
    "# Make sure a <code>config.json</code> has been downloaded and is available in the current working directory\n",
    "\n",
    "# Run through the cells\n",
    "\n",
    "# Verify the pipeline has been created and shows in Azure ML studio, in the <em>Pipelines</em> section\n",
    "\n",
    "# Verify that the pipeline has been scheduled to run or is running\n"
   ]
  },
  {
   "source": [
    "TODO: Please take the following screenshots to show your work:\n",
    "- The pipeline section of Azure ML studio, showing that the pipeline has been created\n",
    "- The pipelines section in Azure ML Studio, showing the Pipeline Endpoint\n",
    "- The Bankmarketing dataset with the AutoML module\n",
    "- The “Published Pipeline overview”, showing a REST endpoint and a status of ACTIVE\n",
    "- In Jupyter Notebook, showing that the “Use RunDetails Widget” shows the step runs\n",
    "- In ML studio showing the scheduled run\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Documentation\n",
    "\n",
    "### Screencast\n",
    "In this project, you need to record a screencast that shows the entire process of the working ML application. The screencast should meet the following criteria: 1-5 min lenght, clear and understandable audio, at least full hd 16:9, readable text.\n",
    "\n",
    "In this project, you need to record a screencast that shows the entire process of the working ML application. The screencast should meet the following criteria:\n",
    "- Working deployed ML model endpoint\n",
    "- deployed pipeline\n",
    "- available automl model\n",
    "- Successful API requests to the endpoint with a JSON payload\n",
    "\n",
    "In case you are unable to provide an audio file, you can include a written description of your script instead of audio, if you prefer. Please include it in your README file.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert link to youtube here"
   ]
  },
  {
   "source": [
    "## Readme\n",
    "An important part of your project submissions is a README file that describes the project and documents the main steps. Please use the README.md template provided to you as a start. The README should include the following areas:\n",
    "\n",
    "- project overview\n",
    "- architectural diagram\n",
    "- short description how to improve project in the future\n",
    "- all screenshots mentioned above with short descriptions\n",
    "- link to the screencast video on youtube (or similar)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert link to readme here"
   ]
  },
  {
   "source": [
    "## Cleanup\n",
    "Not required but included because i think it's important"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cluster.delete()"
   ]
  }
 ]
}